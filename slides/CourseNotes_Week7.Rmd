---
title: "Exploring data 2"
output:
  ioslides_presentation: default
  beamer_presentation:
    theme: metropolis
fontsize: 10pt
---

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(knitr)
library(ggplot2)
library(dplyr)
library(tidyr)
library(ggthemes)
library(faraway)
data(worldcup)
data(nepali)
```

# Matrices

## Matrices

A matrix is like a data frame, but all the values in all columns must be of the same class (e.g., numeric, character).

Matrices can be faster and more memory efficient than data frames.

They are essential for certain mathematical operations.

## Matrices

We can use the `matrix()` function to construct a matrix:

```{r}
foo <- matrix(1:10, ncol = 5)
foo
```

## Matrices

The `as.matrix()` function is used to convert an object to a matrix:

```{r}
foo <- data.frame(col_1 = 1:2, col_2 = 3:4,
                  col_3 = 5:6, col_4 = 7:8,
                  col_5 = 9:10)
foo <- as.matrix(foo)
foo
```

## Matrices

You can index matrices with square brackets, just like data frames: 

```{r}
foo[1, 1:2]
```

You cannot, however, use `dplyr` functions with matrices: 

```{r, eval = FALSE}
foo %>% filter(col_1 == 1)
```

# Lists

## Lists

Lists are the "kitchen sink" of R objects. They can be used to keep together a variety of different R objects of different classes, dimensions, and structures in a single R object. 

Because there are often cases where an R operation results in output that doesn't have a simple structure, lists can be a very useful way to output complex output from an R function. 

Most lists are not "tidy" data. However, we'll cover some ways that you can easily "tidy" some common list objects you might use a lot in your R code, including the output of fitting linear and generalized linear models.

## Lists

```{r}
example_list <- list(a = sample(1:10, 5), 
                     b = data_frame(letters = letters[1:3], 
                                    numbers = 1:3))
example_list
```

## Indexing lists

To pull an element out of a list, you can either use `$` or `[[]]` indexing:

```{r}
example_list$a
```

```{r}
example_list[[2]]
```

## Indexing lists

To access a specific value within a list element we can index the element using double, double brackets:

```{r}
example_list[["b"]][["numbers"]]
```

Again, we can index using names or numeric indices:

```{r}
example_list[["b"]][[1]]
```
## Exploring lists

If an R object is a list, running `class` on the object will return "list": 

```{r}
class(example_list)
```

Often, lists will have names for each element (similar to column names for a dataframe). You can get the names of all elements of a list using the `names` function: 

```{r}
names(example_list)
```

## Exploring lists

The `str` function is also useful for exploring the structure of a list object: 

```{r}
str(example_list)
```

## Lists versus dataframes

As a note, a dataframe is actually just a very special type of list. It is a list where every element (column in the dataframe) is a vector of the same length, and the object has a special attribute specifying that it is a dataframe. 

```{r}
example_df <- data_frame(letters = letters[1:3], 
                         number = 1:3)
class(example_df)
is.list(example_df)
```

# Regression models 

## `nepali` example data

For the `nepali` dataset, each observation is a single measurement for a child; there can be multiple observations per child. \medskip 

I'll limit it to the columns with the child's id, sex, weight, height, and age, and I'll limit to each child's first measurement. 

```{r message = FALSE}
nepali <- nepali %>%
  # Limit to certain columns
  select(id, sex, wt, ht, age) %>%
  # Convert id and sex to factors
  mutate(id = factor(id),
         sex = factor(sex, levels = c(1, 2),
                      labels = c("Male", "Female"))) %>%
  # Limit to first obs. per child
  distinct(id, .keep_all = TRUE)
```

## `nepali` example data

The data now looks like:

```{r}
head(nepali)
```


## Formula structure

*Regression models* can be used to estimate how the expected value of a *dependent variable* changes as *independent variables* change. \medskip

In R, regression formulas take this structure:

```{r eval = FALSE}
## Generic code
[response variable] ~ [indep. var. 1] +  [indep. var. 2] + ...
```

Notice that `~` used to separate the independent and dependent variables and the `+` used to join independent variables. This format mimics the statistical notation:

$$
Y_i \sim X_1 + X_2 + X_3
$$

You will use this type of structure in R for a lot of different function calls, including those for linear models (`lm`) and generalized linear models (`glm`).


## Linear models

To fit a linear model, you can use the function `lm()`. Use the `data` option to specify the dataframe from which to get the vectors. You can save the model as an object. 

```{r}
mod_a <- lm(wt ~ ht, data = nepali)
```

This call fits the model:

$$ Y_{i} = \beta_{0} + \beta_{1}X_{1,i} + \epsilon_{i} $$

where: 

- $Y_{i}$ : weight of child $i$
- $X_{1,i}$ : height of child $i$

## Model objects

The output from fitting a model using `lm` is a list object: 

```{r}
class(mod_a)
```

This list object has a lot of different information from the model, including overall model summaries, estimated coefficients, fitted values, residuals, etc.

```{r}
names(mod_a)
```

## Model objects and `broom`

This list object is not in a "tidy" format. However, there is a package named `broom` that you can use to pull "tidy" dataframes from this model object. 

For example, you can use the `glance` function to pull out a tidy dataframe with model summaries. 

```{r}
library(broom)
glance(mod_a)
```

## Model objects and `broom`

If you want to get the estimated model coefficients (and some related summaries) instead, you can use the `tidy` function to do that: 

```{r}
tidy(mod_a)
```

This output includes, for each model term, the **estimated coefficient** (`estimate`), its **standard error** (`std.error`), the **test statistic** (for `lm` output, the statistic for a test with the null hypothesis that the model coefficient is zero), and the associated **p-value** for that test (`p.value`).

## Model objects and `broom`

Some of the model output have a value for each original observation (e.g., fitted values, residuals). You can use the `augment` function to add those elements to the original data used to fit the model (note: at the moment, you might get a warning message if you run this-- it looks fairly benign and I imagine will be fixed in later versions of the package): 

```{r message = FALSE, warning = FALSE}
augment(mod_a) %>% slice(1:2) %>% select(1:6)
```

## Model objects and `broom`

One important use of this `augment` output is to create a plot with both the original data and a line showing the fit model (via the predictions):

```{r warning = FALSE, message = FALSE, fig.width = 4, fig.height = 2.5, out.width = "0.7\\textwidth", fig.align = "center"}
augment(mod_a) %>%
  ggplot(aes(x = ht, y = wt)) + 
  geom_point(size = 0.8, alpha = 0.8) + 
  geom_line(aes(y = .fitted), color = "red", size = 1.2)
```

## Model objects and `autoplot`

There is a function called `autoplot` in the `ggplot2` package that will check the class of an object and then create a certain default plot for that class. Although the generic `autoplot` function is in the `ggplot2` package, for `lm` and `glm` objects, you must have the `ggfortify` package installed and loaded to be able to access the methods of `autoplot` specifically for these object types. 

If you have the package that includes an `autoplot` method for a specific object type, you can just run `autoplot` on the objects name and get a plot that is considered a useful default for that object type. For `lm` objects, `autoplot` gives small graphics with model diagnostic plots.

## Model objects and `autoplot`

```{r out.width = "0.8\\textwidth", fig.align = "center"}
library(ggfortify)
autoplot(mod_a)
```

## Model objects and `autoplot`

The output from `autoplot` is a `ggplot` object, so you can add elements to it as you would with other `ggplot` objects:

```{r out.width = "0.7\\textwidth", fig.align = "center"}
autoplot(mod_a) + 
  theme_classic()
```

## Model objects and base R "method" functions

Here, I've focused on a "tidy" approach to working with the output from fitting an `lm` model. However, you are certain to come across base R functions to work with this output in a similar way. Some base R functions you can use on model objects:

```{r echo = FALSE}
mod_objects <- data.frame(Function = c("`summary`", "`coef`", 
                                   "`fitted`",
                                   "`plot`", "`residuals`"),
                          Description = c("Get a variety of information on the model, including coefficients and p-values for the coefficients",
                                   "Pull out just the coefficients for a model",
                                   "Get the fitted values from the model (for the data used to fit the model)",
                                   "Create plots to help assess model assumptions",
                                   "Get the model residuals"))
pander::pander(mod_objects, split.cells = c(1,1,58),
               justify = c("center", "left"))
```

## Examples of using a model object

One base R function you should definitely know is the `summary` function. The `summary` function gives you a lot of information about the model: 

```{r, eval = FALSE}
summary(mod_a)
```

(see next slide)

***

```{r, echo = FALSE}
summary(mod_a)
```

## In-course exercise

We'll take a break now to do part of the In-Course Exercise (Section 7.6.1).

## Fitting a model with a factor

You can also use binary variables or factors as independent variables in regression models:

```{r}
mod_b <- lm(wt ~ sex, data = nepali)
tidy(mod_b)
```

This call fits the model:

$$ Y_{i} = \beta_{0} + \beta_{1}X_{1,i} + \epsilon_{i} $$

where $X_{1,i}$ : sex of child $i$, where 0 = male; 1 = female

## Linear models versus GLMs

You can fit a variety of models, including linear models, logistic models, and Poisson models, using generalized linear models (GLMs). \medskip

For linear models, the only difference between `lm` and `glm` is how they're fitting the model (least squares versus maximum likelihood). You should get the same results regardless of which you pick. 

## Linear models versus GLMs

For example:

```{r}
mod_c <- glm(wt ~ ht, data = nepali)
tidy(mod_c)
tidy(mod_a)
```

## GLMs

You can fit other model types with `glm()` using the `family` option:

```{r echo = FALSE}
glm_types <- data.frame(type = c("Linear", "Logistic", "Poisson"),
                        opt = c("`family = gaussian(link = 'identity')`",
                                "`family = binomial(link = 'logit')`", 
                                "`family = poisson(link = 'log')`"))
knitr::kable(glm_types, col.names = c("Model type", "`family` option"))
```

## Logistic example

For example, say we wanted to fit a logistic regression for the `nepali` data of whether the probability that a child weighs more than 13 kg is associated with the child's height. \medskip

First, create a binary variable for `wt_over_13`:

```{r}
nepali <- nepali %>% 
  mutate(wt_over_13 = wt > 13)
head(nepali)
```

## Logistic example

Now you can fit a logistic regression:

```{r}
mod_d <- glm(wt_over_13 ~ ht, data = nepali,
             family = binomial(link = "logit"))
tidy(mod_d)
```

Here, the model coefficient gives the **log odds** of having a weight higher than 13 kg associated with a unit increase in height.

## Formula structure

There are some conventions that can be used in R formulas. Common ones include: 

```{r echo = FALSE}
for_convs <- data.frame(Convention = c("`I()`", "`:`", "`*`", "`.`",
                                       "`-`", "`1`"),
                        Meaning = c("calculate the value inside before fitting (e.g., `I(x1 + x2)`)",
                                    "fit the interaction between two variables (e.g., `x1:x2`)",
                                    "fit the main effects and interaction for both variables (e.g., `x1*x2` equals `x1 + x2 + x1:x2`)",
                                    "fit all variables other than the response (e.g., `y ~ .`)",
                                    "do not include a variable (e.g., `y ~ . - x1`)",
                                    "intercept (e.g., `y ~ 1`)"))
pander::pander(for_convs, split.cells = c(1,1,58),
               justify = c("center", "left"))
```

## To find out more

Great resources to find out more about using R for basic statistics:

- Statistical Analysis with R for Dummies, Joseph Schmuller (free online through our library; Chapter 14 covers regression modeling)
- The R Book, Michael J. Crawley (free online through our library; Chapter 14 covers regression modeling, Chapters 10 and 13 cover linear and generalized linear regression modeling)
- R for Data Science (Section 4)

If you want all the details about fitting linear models and GLMs in R, Faraway's books are fantastic (more at level of Master's in Applied Statistics):

- Linear Models with R, Julian Faraway (also freely available online through our library)
- Extending the Linear Model with R, Julian Faraway (available in hardcopy through our library)

## In-course exercise

We'll take a break now to do part of the In-Course Exercise (Section 7.6.2).

