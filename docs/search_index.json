[
["entering-and-cleaning-data-3.html", "Chapter 11 Entering and cleaning data #3 11.1 Pulling online data 11.2 Example R API wrapper packages 11.3 tigris package 11.4 countyweather 11.5 Cleaning very messy data 11.6 In-course exercise", " Chapter 11 Entering and cleaning data #3 Download a pdf of the lecture slides covering this topic. 11.1 Pulling online data 11.1.1 APIs APIs are “Application Program Interfaces”. An API provides the rules for software applications to interact. In the case of open data APIs, they provide the rules you need to know to write R code to request and pull data from the organization’s web server into your R session. Often, an API can help you avoid downloading all available data, and instead only download the subset you need. The basic strategy for using APIs from R is: Figure out the API rules for HTTP requests Write R code to create a request in the proper format Send the request using GET or POST HTTP methods Once you get back data from the request, parse it into an easier-to-use format if necessary Start by reading any documentation available for the API. This will often give information on what data is available and how to put together requests. Source: https://api.nasa.gov/api.html#EONET Many organizations will require you to get an API key and use this key in each of your API requests. This key allows the organization to control API access, including enforcing rate limits per user. API rate limits restrict how often you can request data (e.g., an hourly limit of 1,000 requests per user for NASA APIs). You should keep this key private. In particular, make sure you do not include it in code that is posted to GitHub. The riem package, developed by Maelle Salmon and an ROpenSci package, is an excellent and straightforward example of how you can use R to pull open data through a web API. This package allows you to pull weather data from airports around the world directly from the Iowa Environmental Mesonet. To get a certain set of weather data from the Iowa Environmental Mesonet, you can send an HTTP request specifying a base URL, “https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py/”, as well as some parameters describing the subset of dataset you want (e.g., date ranges, weather variables, output format). Once you know the rules for the names and possible values of these parameters (more on that below), you can submit an HTTP GET request using the GET function from the httr package. https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?station=DEN&amp;data=sknt&amp;year1=2016&amp;month1=6&amp;day1=1&amp;year2=2016&amp;month2=6&amp;day2=30&amp;tz=America%2FDenver&amp;format=comma&amp;latlon=no&amp;direct=no&amp;report_type=1&amp;report_type=2 When you are making an HTTP request using the GET or POST functions from the httr package, you can include the key-value pairs for any query parameters as a list object in the query argurment of the function. library(httr) meso_url &lt;- paste0(&quot;https://mesonet.agron.iastate.edu/&quot;, &quot;cgi-bin/request/asos.py/&quot;) denver &lt;- GET(url = meso_url, query = list(station = &quot;DEN&quot;, data = &quot;sped&quot;, year1 = &quot;2016&quot;, month1 = &quot;6&quot;, day1 = &quot;1&quot;, year2 = &quot;2016&quot;, month2 = &quot;6&quot;, day2 = &quot;30&quot;, tz = &quot;America/Denver&quot;, format = &quot;comma&quot;)) The GET call will return a special type of list object with elements that include the url you queried and the content of the page at that url: str(denver, max.level = 1, list.len = 6) ## List of 10 ## $ url : chr &quot;https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py/?station=DEN&amp;data=sped&amp;year1=2016&amp;month1=6&amp;day1=1&amp;yea&quot;| __truncated__ ## $ status_code: int 200 ## $ headers :List of 6 ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;insensitive&quot; &quot;list&quot; ## $ all_headers:List of 1 ## $ cookies :&#39;data.frame&#39;: 0 obs. of 7 variables: ## $ content : raw [1:230654] 23 44 45 42 ... ## [list output truncated] ## - attr(*, &quot;class&quot;)= chr &quot;response&quot; The httr package includes functions to pull out elements of this list object, including: headers: Pull out the header information content: Pull out the content returned from the page status_code: Pull out the status code from the GET request (e.g., 200: okay; 404: not found) Note: For some fun examples of 404 pages, see https://www.creativebloq.com/web-design/best-404-pages-812505 You can use content from httr to retrieve the contents of the HTTP request we made. For this particular web data, the requested data is a comma-separated file, so you can convert it to a dataframe with read_csv: denver %&gt;% content() %&gt;% read_csv(skip = 5, na = &quot;M&quot;) %&gt;% slice(1:3) ## # A tibble: 3 x 3 ## station valid sped ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 DEN 2016-06-01 01:00:00 6.9 ## 2 DEN 2016-06-01 01:05:00 6.9 ## 3 DEN 2016-06-01 01:10:00 6.9 The riem package wraps up this whole process, so you can call a single function to get in the data you want from the API: library(riem) denver_2 &lt;- riem_measures(station = &quot;DEN&quot;, date_start = &quot;2016-06-01&quot;, date_end = &quot;2016-06-30&quot;) denver_2 %&gt;% slice(1:3) ## # A tibble: 3 x 24 ## station valid lon lat tmpf dwpf relh drct sknt ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 DEN 2016-06-01 00:00:00 -105. 39.8 NA NA NA 70 7 ## 2 DEN 2016-06-01 00:05:00 -105. 39.8 NA NA NA 80 8 ## 3 DEN 2016-06-01 00:10:00 -105. 39.8 NA NA NA 80 9 ## # ... with 15 more variables: p01i &lt;dbl&gt;, alti &lt;dbl&gt;, mslp &lt;dbl&gt;, ## # vsby &lt;dbl&gt;, gust &lt;dbl&gt;, skyc1 &lt;chr&gt;, skyc2 &lt;chr&gt;, skyc3 &lt;chr&gt;, ## # skyc4 &lt;chr&gt;, skyl1 &lt;dbl&gt;, skyl2 &lt;dbl&gt;, skyl3 &lt;dbl&gt;, skyl4 &lt;dbl&gt;, ## # wxcodes &lt;chr&gt;, metar &lt;chr&gt; 11.2 Example R API wrapper packages The tigris package is a very useful example of an API wrapper. It retrieves geographic boundary data from the U.S. Census for a number of different geographies: Location boundaries States Counties Blocks Tracks School districts Congressional districts Roads Primary roads Primary and secondary roads Water Area-water Linear-water Coastline Other Landmarks Military 11.3 tigris package The following plot is an example of the kinds of maps you can create using the tigris package. This map comes from: Kyle Walker. 2016. “tigris: An R Package to Access and Work with Geographic Data from the US Census Bureau”. The R Journal. This is a great article to read to find out more about tigris. A number of other R packages also help you access and use data from the U.S. Census: acs: Download, manipulate, and present American Community Survey and Decennial data from the US Census (see “Working with the American Community Survey in R: A Guide to Using the acs Package”, a book available free online through the CSU library) USABoundaries: Historical and contemporary boundaries of the United States of America idbr: R interface to the US Census Bureau International Data Base API (e.g., populations of other countries) The organization rOpenSci (https://ropensci.org) has the following mission: “At rOpenSci we are creating packages that allow access to data repositories through the R statistical programming environment that is already a familiar part of the workflow of many scientists. Our tools not only facilitate drawing data into an environment where it can readily be manipulated, but also one in which those analyses and methods can be easily shared, replicated, and extended by other researchers.” rOpenSci collects a number of packages for tapping into open data for research. These are listed at https://ropensci.org/packages. Many of these packages are wrappers for APIs with data useful for scientific research Some examples (all descriptions from rOpenSci): AntWeb: Access data from the world’s largest ant database chromer: Interact with the chromosome counts database (CCDB) gender: Encodes gender based on names and dates of birth musemeta: R Client for Scraping Museum Metadata, including The Metropolitan Museum of Art, the Canadian Science &amp; Technology Museum Corporation, the National Gallery of Art, and the Getty Museum, and more to come. rusda: Interface to some USDA databases webchem: Retrieve chemical information from many sources. Currently includes: Chemical Identifier Resolver, ChemSpider, PubChem, and Chemical Translation Service. As an example, one ROpenSci package, rnoaa, allows you to: “Access climate data from NOAA, including temperature and precipitation, as well as sea ice cover data, and extreme weather events” It includes access to: Buoy data from the National Buoy Data Center Historical Observing Metadata Repository (HOMR))— climate station metadata National Climatic Data Center weather station data Sea ice data International Best Track Archive for Climate Stewardship (IBTrACS)— tropical cyclone tracking data Severe Weather Data Inventory (SWDI) 11.4 countyweather The countyweather package, developed by a student here at CSU, wraps the rnoaa package to let you pull and aggregate weather at the county level in the U.S. For example, you can pull all data from Miami during Hurricane Andrew: When you pull the data for a county, the package also maps the contributing weather stations: The USGS also has a very nice collection of R packages that wrap USGS open data APIs, which can be accessed through: https://owi.usgs.gov/R/ “USGS-R is a community of support for users of the R scientific programming language. USGS-R resources include R training materials, R tools for the retrieval and analysis of USGS data, and support for a growing group of USGS-R developers.” USGS R packages include: dataRetrieval: Obtain water quality sample data, streamflow data, and metadata directly from either the USGS or EPA EGRET: Analysis of long-term changes in water quality and streamflow, including the water-quality method Weighted Regressions on Time, Discharge, and Season (WRTDS) laketemps: Lake temperature data package for Global Lake Temperature Collaboration Project lakeattributes: Common useful lake attribute data soilmoisturetools: Tools for soil moisture data retrieval and visualization Here are some examples of other R packages that faciliate use of an API for open data: twitteR: Twitter Quandl: Quandl (financial data) RGoogleAnalytics: Google Analytics WDI, wbstats: World Bank GuardianR, rdian: The Guardian Media Group blsAPI: Bureau of Labor Statistics rtimes: New York Times Find out more about writing API packages with this vignette for the httr package: https://cran.r-project.org/web/packages/httr/vignettes/api-packages.html. This document includes advice on error handling within R code that accesses data through an open API. 11.5 Cleaning very messy data One version of Atlantic basin hurricane tracks is available here: https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2017-050118.txt. The data is not in a classic delimited format: This data is formatted in the following way: Data for many storms are included in one file. Data for a storm starts with a shorter line, with values for the storm ID, name, and number of observations for the storm. These values are comma separated. Observations for each storm are longer lines. There are multiple observations for each storm, where each observation gives values like the location and maximum winds for the storm at that time. Strategy for reading in very messy data: Read in all lines individually. Use regular expressions to split each line into the elements you’d like to use to fill columns. Write functions and / or map calls to process lines and use the contents to fill a data frame. Once you have the data in a data frame, do any remaining cleaning to create a data frame that is easy to use to answer research questions. Because the data is not nicely formatted, you can’t use read_csv or similar functions to read it in. However, the read_lines function from readr allows you to read a text file in one line at a time. You can then write code and functions to parse the file one line at a time, to turn it into a dataframe you can use. Note: Base R has readLines, which is very similar. The read_lines function from readr will read in lines from a text file directly, without trying to separate into columns. You can use the n_max argument to specify the number of lines to read it. For example, to read in three lines from the hurricane tracking data, you can run: tracks_url &lt;- paste0(&quot;http://www.nhc.noaa.gov/data/hurdat/&quot;, &quot;hurdat2-1851-2017-050118.txt&quot;) hurr_tracks &lt;- read_lines(tracks_url, n_max = 3) hurr_tracks ## [1] &quot;AL011851, UNNAMED, 14,&quot; ## [2] &quot;18510625, 0000, , HU, 28.0N, 94.8W, 80, -999, -999, -999, -999, -999, -999, -999, -999, -999, -999, -999, -999, -999,&quot; ## [3] &quot;18510625, 0600, , HU, 28.0N, 95.4W, 80, -999, -999, -999, -999, -999, -999, -999, -999, -999, -999, -999, -999, -999,&quot; The data has been read in as a vector, rather than a dataframe: class(hurr_tracks) ## [1] &quot;character&quot; length(hurr_tracks) ## [1] 3 hurr_tracks[1] ## [1] &quot;AL011851, UNNAMED, 14,&quot; You can use regular expressions to break each line up. For example, you can use str_split from the stringr package to break the first line of the hurricane track data into its three separate components: library(stringr) str_split(hurr_tracks[1], pattern = &quot;,&quot;) ## [[1]] ## [1] &quot;AL011851&quot; &quot; UNNAMED&quot; &quot; 14&quot; ## [4] &quot;&quot; You can use this to create a list where each element of the list has the split-up version of a line of the original data. First, read in all of the data: tracks_url &lt;- paste0(&quot;http://www.nhc.noaa.gov/data/hurdat/&quot;, &quot;hurdat2-1851-2017-050118.txt&quot;) hurr_tracks &lt;- read_lines(tracks_url) length(hurr_tracks) ## [1] 52151 Next, use map with str_split to split each line of the data at the commas: library(purrr) hurr_tracks &lt;- map(hurr_tracks, str_split, pattern = &quot;,&quot;, simplify = TRUE) hurr_tracks[[1]] ## [,1] [,2] [,3] [,4] ## [1,] &quot;AL011851&quot; &quot; UNNAMED&quot; &quot; 14&quot; &quot;&quot; hurr_tracks[[2]][1:2] ## [1] &quot;18510625&quot; &quot; 0000&quot; Next, you want to split this list into two lists, one with the shorter “meta-data” lines and one with the longer “observation” lines. You can use map_int to create a vector with the length of each line. You will later use this to identify which lines are short or long. hurr_lengths &lt;- map_int(hurr_tracks, length) hurr_lengths[1:17] ## [1] 4 21 21 21 21 21 21 21 21 21 21 21 21 21 21 4 21 unique(hurr_lengths) ## [1] 4 21 You can use bracket indexing to split the hurr_tracks into two lists: one with the shorter lines that start each observation (hurr_meta) and one with the storm observations (hurr_obs). Use bracket indexing with the hurr_lengths vector you just created to make that split. hurr_meta &lt;- hurr_tracks[hurr_lengths == 4] hurr_obs &lt;- hurr_tracks[hurr_lengths == 21] hurr_meta[1:3] ## [[1]] ## [,1] [,2] [,3] [,4] ## [1,] &quot;AL011851&quot; &quot; UNNAMED&quot; &quot; 14&quot; &quot;&quot; ## ## [[2]] ## [,1] [,2] [,3] [,4] ## [1,] &quot;AL021851&quot; &quot; UNNAMED&quot; &quot; 1&quot; &quot;&quot; ## ## [[3]] ## [,1] [,2] [,3] [,4] ## [1,] &quot;AL031851&quot; &quot; UNNAMED&quot; &quot; 1&quot; &quot;&quot; hurr_obs[1:2] ## [[1]] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] &quot;18510625&quot; &quot; 0000&quot; &quot; &quot; &quot; HU&quot; &quot; 28.0N&quot; &quot; 94.8W&quot; &quot; 80&quot; &quot; -999&quot; ## [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] ## [1,] &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; ## [,17] [,18] [,19] [,20] [,21] ## [1,] &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot;&quot; ## ## [[2]] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] &quot;18510625&quot; &quot; 0600&quot; &quot; &quot; &quot; HU&quot; &quot; 28.0N&quot; &quot; 95.4W&quot; &quot; 80&quot; &quot; -999&quot; ## [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] ## [1,] &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; ## [,17] [,18] [,19] [,20] [,21] ## [1,] &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot;&quot; Now, you can use bind_rows from dplyr to change the list of metadata into a dataframe. (You first need to use as_tibble with map to convert all elements of the list from matrices to dataframes.) library(dplyr); library(tibble) hurr_meta &lt;- hurr_meta %&gt;% map(as_tibble) %&gt;% bind_rows() hurr_meta %&gt;% slice(1:3) ## # A tibble: 3 x 4 ## V1 V2 V3 V4 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AL011851 &quot; UNNAMED&quot; &quot; 14&quot; &quot;&quot; ## 2 AL021851 &quot; UNNAMED&quot; &quot; 1&quot; &quot;&quot; ## 3 AL031851 &quot; UNNAMED&quot; &quot; 1&quot; &quot;&quot; You can clean up the data a bit more. First, the fourth column doesn’t have any non-missing values, so you can get rid of it: unique(hurr_meta$V4) ## [1] &quot;&quot; Second, the second and third columns include a lot of leading whitespace: hurr_meta$V2[1:2] ## [1] &quot; UNNAMED&quot; &quot; UNNAMED&quot; Last, we want to name the columns. hurr_meta &lt;- hurr_meta %&gt;% select(-V4) %&gt;% rename(storm_id = V1, storm_name = V2, n_obs = V3) %&gt;% mutate(storm_name = str_trim(storm_name), n_obs = as.numeric(n_obs)) hurr_meta %&gt;% slice(1:3) ## # A tibble: 3 x 3 ## storm_id storm_name n_obs ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 AL011851 UNNAMED 14 ## 2 AL021851 UNNAMED 1 ## 3 AL031851 UNNAMED 1 Now you can do the same idea with the hurricane observations. First, we’ll want to add storm identifiers to that data. The “meta” data includes storm ids and the number of observations per storm. We can take advantage of that to make a storm_id vector that will line up with the storm observations. storm_id &lt;- rep(hurr_meta$storm_id, times = hurr_meta$n_obs) head(storm_id, 3) ## [1] &quot;AL011851&quot; &quot;AL011851&quot; &quot;AL011851&quot; length(storm_id) ## [1] 50303 length(hurr_obs) ## [1] 50303 hurr_obs &lt;- hurr_obs %&gt;% map(as_tibble) %&gt;% bind_rows() %&gt;% mutate(storm_id = storm_id) hurr_obs %&gt;% select(V1:V2, V5:V6, storm_id) %&gt;% slice(1:3) ## # A tibble: 3 x 5 ## V1 V2 V5 V6 storm_id ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 18510625 &quot; 0000&quot; &quot; 28.0N&quot; &quot; 94.8W&quot; AL011851 ## 2 18510625 &quot; 0600&quot; &quot; 28.0N&quot; &quot; 95.4W&quot; AL011851 ## 3 18510625 &quot; 1200&quot; &quot; 28.0N&quot; &quot; 96.0W&quot; AL011851 11.6 In-course exercise 11.6.1 Working with an API wrapper package The rplos package provides a wrapper to the Public Library of Science (PLoS)’s API. PLOS has a collection of academic journals spanning a variety of topics. Check out this page of documentation for this API: http://api.plos.org/solr/search-fields/ Look through the potential search terms. Use the searchplos function to search articles in the PLoS collection for the term “West Nile”. Pull the publication date, title, abstract, article type, subject, and journal of each matching article and save the result to an R object called wn_papers. You may find it helpful to look at the examples in the helpfile for searchplos or the tutorial available at: https://ropensci.org/tutorials/rplos_tutorial/ The object returned by searchplos will be a list with two top levels, meta and data. Confirm that this is true for the wn_papers object you created. Look at the meta part of the list (you can use $ indexing to pull this out). How many articles were found with “West Nile” in them? Does the query seem case-sensitive (i.e., do you get the same number of papers when you query “west nile” rather than “West Nile”)? Re-run your query (save the results to wn_papers_titles) looking only for papers with “West Nile” in the title of the paper. How many papers are returned by this query? By default the limit of the number of papers returned by a query will be 10. You can change this (to a certain degree) by using the limit option. For the call you ran to create wn_papers_titles, set the limit to the number of articles that match this query, as identified in the meta element of the first run of the call. Check the number of rows in the data element that was returned to make sure it has the same number of rows as the number of articles that match the query. Create a plot of the number of articles published per year. Use color to show which articles are Research Articles versus other types of articles. Determine which journals have published these articles and the number of articles published in each journal. You may notice that sometimes “PLoS” is used and sometimes “PLOS”. See if you can fix this in R to get the count of articles per journal without this capitalization difference causing problems. Explore the list of packages on ROpenSci, those through the USGS, and those related to the U.S. Census. See if you can identify any other packages that might provide access to data relevant to West Nile virus. 11.6.1.1 Example R code library(rplos) wn_papers &lt;- searchplos(q = &quot;West Nile&quot;, fl = c(&quot;publication_date&quot;, &quot;title&quot;, &quot;journal&quot;, &quot;subject&quot;, &quot;abstract&quot;, &quot;article_type&quot;)) Confirm the structure of the returned object: str(wn_papers) ## List of 2 ## $ meta:Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 1 obs. of 2 variables: ## ..$ numFound: int 2873 ## ..$ start : int 0 ## $ data:Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 10 obs. of 6 variables: ## ..$ subject : chr [1:10] &quot;/Earth sciences/Geography/Regional geography/Geographical regions/Mediterranean Basin,/Medicine and health scie&quot;| __truncated__ &quot;/Biology and life sciences/Genetics/Genomics/Animal genomics/Mammalian genomics,/Biology and life sciences/Micr&quot;| __truncated__ &quot;/Social sciences/Economics/Health economics,/Medicine and health sciences/Health care/Health economics,/Biology&quot;| __truncated__ &quot;/Earth sciences/Geography/Human geography/Land use,/Earth sciences/Atmospheric science/Meteorology,/People and &quot;| __truncated__ ... ## ..$ journal : chr [1:10] &quot;PLOS ONE&quot; &quot;PLOS Neglected Tropical Diseases&quot; &quot;PLoS ONE&quot; &quot;PLOS ONE&quot; ... ## ..$ publication_date: chr [1:10] &quot;2015-12-30T00:00:00Z&quot; &quot;2017-11-08T00:00:00Z&quot; &quot;2010-11-12T00:00:00Z&quot; &quot;2015-03-24T00:00:00Z&quot; ... ## ..$ article_type : chr [1:10] &quot;Research Article&quot; &quot;Research Article&quot; &quot;Research Article&quot; &quot;Research Article&quot; ... ## ..$ abstract : chr [1:10] &quot;\\nWest Nile virus (WNV) is a mosquito-transmitted Flavivirus belonging to the Japanese encephalitis antigenic c&quot;| __truncated__ &quot;\\nThe West Nile virus (WNV), isolated in 1937, is an arbovirus (arthropod-borne virus) that infects thousands o&quot;| __truncated__ &quot;\\n Understanding the conditions underlying the proliferation of infectious diseases is crucial for mitig&quot;| __truncated__ &quot;\\nWest Nile Virus (WNV) is a globally important mosquito borne virus, with significant implications for human a&quot;| __truncated__ ... ## ..$ title : chr [1:10] &quot;Spatio-Temporal Identification of Areas Suitable for West Nile Disease in the Mediterranean Basin and Central Europe&quot; &quot;Biological and phylogenetic characteristics of West African lineages of West Nile virus&quot; &quot;Economic Conditions Predict Prevalence of West Nile Virus&quot; &quot;Identifying the Environmental Conditions Favouring West Nile Virus Outbreaks in Europe&quot; ... ## ..- attr(*, &quot;numFound&quot;)= int 2873 ## ..- attr(*, &quot;start&quot;)= int 0 Look at the meta data: wn_papers$meta ## # A tibble: 1 x 2 ## numFound start ## &lt;int&gt; &lt;int&gt; ## 1 2873 0 Query for just articles with “West Nile” in the title: wn_papers_titles &lt;- searchplos(q = &quot;title:West Nile&quot;, fl = c(&quot;publication_date&quot;, &quot;title&quot;, &quot;journal&quot;, &quot;subject&quot;, &quot;abstract&quot;, &quot;article_type&quot;)) Look at the metadata: wn_papers_titles$meta ## # A tibble: 1 x 2 ## numFound start ## &lt;int&gt; &lt;int&gt; ## 1 191 0 Re-run the query using an appropriate limit to get all the matching articles: wn_papers_titles &lt;- searchplos(q = &quot;title:West Nile&quot;, fl = c(&quot;publication_date&quot;, &quot;title&quot;, &quot;journal&quot;, &quot;subject&quot;, &quot;abstract&quot;, &quot;article_type&quot;), limit = 190) Check the number of rows: nrow(wn_papers_titles$data) ## [1] 190 Create a plot of the number of articles in the PLoS journals with “West Nile” in the title by year: library(lubridate) library(dplyr) library(ggplot2) wn_papers_titles$data %&gt;% mutate(publication_date = ymd_hms(publication_date), pub_year = year(publication_date), research_article = article_type == &quot;Research Article&quot;) %&gt;% group_by(pub_year, research_article) %&gt;% count() %&gt;% ggplot(aes(x = pub_year, y = n, fill = research_article)) + geom_col() + labs(x = &quot;Year of publication&quot;, y = &quot;Number of articles published&quot;, fill = &quot;Research article&quot;) + ggtitle(&quot;West Nile articles published in PLoS journals&quot;, subtitle = &quot;Based on articles with &#39;West Nile&#39; in the title&quot;) Determine which journals have published these articles and the number of articles published in each journal: library(forcats) library(stringr) wn_papers_titles$data %&gt;% mutate(journal = str_replace(journal, &quot;PLOS&quot;, &quot;PLoS&quot;)) %&gt;% group_by(journal) %&gt;% count() %&gt;% ungroup() %&gt;% filter(!is.na(journal)) %&gt;% arrange(desc(n)) ## # A tibble: 6 x 2 ## journal n ## &lt;chr&gt; &lt;int&gt; ## 1 PLoS ONE 110 ## 2 PLoS Neglected Tropical Diseases 38 ## 3 PLoS Pathogens 27 ## 4 PLoS Medicine 3 ## 5 PLoS Biology 2 ## 6 PLoS Computational Biology 2 11.6.2 Cleaning very messy data With your groups, create an R script that does all the steps described so far to pull the messy hurricane tracks data from online and clean it. Then try the following further cleaning steps: Select only the columns with date, time, storm status, location (latitude and longitude), maximum sustained winds, and minimum pressure and renames them Create a column with the date-time of each observation, in a date-time class Clean up the latitude and longitude so that you have separate columns for the numeric values and for the direction indicator (e.g., N, S, E, W) Clean up the wind column, so it gives wind speed as a number and NA in cases where wind speed is missing If you have time, try to figure out what the status abbreviations stand for. Create a new factor column named status_long with the status spelled out. 11.6.3 Exploring network data The following code offers some examples of exploring the PLoS data using a network framework. library(stringr) library(tidyr) paper_subject &lt;- wn_papers_titles$data %&gt;% mutate(id = 1:n()) %&gt;% select(id, subject) %&gt;% mutate(subjects = str_split(subject, &quot;/&quot;)) %&gt;% select(-subject) %&gt;% unnest(subjects) %&gt;% filter(subjects != &quot;&quot;) %&gt;% mutate(subjects = str_replace(subjects, &quot;,&quot;, &quot;&quot;)) head(paper_subject) ## # A tibble: 6 x 2 ## id subjects ## &lt;int&gt; &lt;chr&gt; ## 1 1 Biology and life sciences ## 2 1 Genetics ## 3 1 Genomics ## 4 1 Animal genomics ## 5 1 Mammalian genomics ## 6 1 Biology and life sciences Here are the top ten listed subjects: paper_subject %&gt;% group_by(subjects) %&gt;% count() %&gt;% ungroup() %&gt;% arrange(desc(n)) %&gt;% slice(1:10) ## # A tibble: 10 x 2 ## subjects n ## &lt;chr&gt; &lt;int&gt; ## 1 Biology and life sciences 1946 ## 2 Medicine and health sciences 899 ## 3 Flaviviruses 796 ## 4 West Nile virus 672 ## 5 Viral pathogens 636 ## 6 Organisms 630 ## 7 Viruses 434 ## 8 Microbial pathogens 426 ## 9 Immunology 342 ## 10 Microbiology 294 library(widyr) subject_pairs &lt;- paper_subject %&gt;% pairwise_count(item = subjects, feature = id) %&gt;% rename(from = item1, to = item2, weight = n) head(subject_pairs) ## # A tibble: 6 x 3 ## from to weight ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Genetics Biology and life sciences 44 ## 2 Genomics Biology and life sciences 13 ## 3 Animal genomics Biology and life sciences 7 ## 4 Mammalian genomics Biology and life sciences 2 ## 5 Microbiology Biology and life sciences 175 ## 6 Virology Biology and life sciences 57 library(tidygraph) subject_network &lt;- subject_pairs %&gt;% filter(weight &gt; 50) %&gt;% as_tbl_graph(directed = FALSE) subject_network ## # A tbl_graph: 34 nodes and 774 edges ## # ## # An undirected multigraph with 1 component ## # ## # Node Data: 34 x 1 (active) ## name ## &lt;chr&gt; ## 1 Microbiology ## 2 Virology ## 3 Organisms ## 4 Viruses ## 5 Viral pathogens ## 6 Flaviviruses ## # ... with 28 more rows ## # ## # Edge Data: 774 x 3 ## from to weight ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 34 175 ## 2 2 34 57 ## 3 3 34 182 ## # ... with 771 more rows library(ggraph) library(viridis) ## Loading required package: viridisLite subject_network %&gt;% mutate(centrality = centrality_authority(), subject_group = as.factor(group_infomap())) %&gt;% ggraph(layout = &#39;kk&#39;) + geom_edge_link(aes(edge_alpha = weight), show.legend = FALSE) + geom_node_label(aes(label = name, fill = subject_group, size = centrality), color = &quot;white&quot;, show.legend = FALSE, repel = TRUE) + scale_fill_viridis(discrete = TRUE, option = &quot;B&quot;) + theme_graph() subject_cors &lt;- paper_subject %&gt;% group_by(subjects) %&gt;% filter(n() &gt;= 40) %&gt;% pairwise_cor(item = subjects, feature = id) %&gt;% filter(correlation &gt; .4) %&gt;% as_tbl_graph(directed = FALSE) subject_cors %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = &quot;royalblue&quot;) + geom_node_point(size = 5) + geom_node_text(aes(label = name), repel = TRUE, point.padding = unit(0.2, &quot;lines&quot;)) + theme_void() "]
]
