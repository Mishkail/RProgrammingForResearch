[
["exploring-data-2.html", "Chapter 7 Exploring data #2 7.1 Simple statistical tests in R 7.2 Matrices 7.3 Lists 7.4 Regression models 7.5 Handling model objects 7.6 In-course exercise", " Chapter 7 Exploring data #2 Download a pdf of the lecture slides covering this topic. 7.1 Simple statistical tests in R Let’s pull the fatal accident data just for the county that includes Las Vegas, NV. Each US county has a unique identifier (FIPS code), composed of a two-digit state FIPS and a three-digit county FIPS code. The state FIPS for Nevada is 32; the county FIPS for Clark County is 003. Therefore, we can filter down to Clark County data in the FARS data we collected with the following code: library(readr) library(dplyr) clark_co_accidents &lt;- read_csv(&quot;data/accident.csv&quot;) %&gt;% filter(STATE == 32 &amp; COUNTY == 3) We can also check the number of accidents: clark_co_accidents %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 201 We want to test if the probability, on a Friday or Saturday, of a fatal accident occurring is higher than on other days of the week. Let’s clean the data up a bit as a start: library(tidyr) library(lubridate) clark_co_accidents &lt;- clark_co_accidents %&gt;% select(DAY, MONTH, YEAR) %&gt;% unite(date, DAY, MONTH, YEAR, sep = &quot;-&quot;) %&gt;% mutate(date = dmy(date)) Here’s what the data looks like now: clark_co_accidents %&gt;% slice(1:5) ## # A tibble: 5 x 1 ## date ## &lt;date&gt; ## 1 2016-01-10 ## 2 2016-02-21 ## 3 2016-01-06 ## 4 2016-01-13 ## 5 2016-02-18 Next, let’s get the count of accidents by date: clark_co_accidents &lt;- clark_co_accidents %&gt;% group_by(date) %&gt;% count() %&gt;% ungroup() clark_co_accidents %&gt;% slice(1:3) ## # A tibble: 3 x 2 ## date n ## &lt;date&gt; &lt;int&gt; ## 1 2016-01-03 1 ## 2 2016-01-06 1 ## 3 2016-01-09 3 We’re missing the dates without a fatal crash, so let’s add those. First, create a dataframe with all dates in 2016: all_dates &lt;- data_frame(date = seq(ymd(&quot;2016-01-01&quot;), ymd(&quot;2016-12-31&quot;), by = 1)) all_dates %&gt;% slice(1:5) ## # A tibble: 5 x 1 ## date ## &lt;date&gt; ## 1 2016-01-01 ## 2 2016-01-02 ## 3 2016-01-03 ## 4 2016-01-04 ## 5 2016-01-05 Then merge this with the original dataset on Las Vegas fatal crashes and make any day missing from the fatal crashes dataset have a “0” for number of fatal accidents (n): clark_co_accidents &lt;- clark_co_accidents %&gt;% right_join(all_dates, by = &quot;date&quot;) %&gt;% # If `n` is missing, set to 0. Otherwise keep value. mutate(n = ifelse(is.na(n), 0, n)) clark_co_accidents %&gt;% slice(1:3) ## # A tibble: 3 x 2 ## date n ## &lt;date&gt; &lt;dbl&gt; ## 1 2016-01-01 0. ## 2 2016-01-02 0. ## 3 2016-01-03 1. Next, let’s add some information about day of week and weekend: clark_co_accidents &lt;- clark_co_accidents %&gt;% mutate(weekday = wday(date, label = TRUE), weekend = weekday %in% c(&quot;Fri&quot;, &quot;Sat&quot;)) clark_co_accidents %&gt;% slice(1:3) ## # A tibble: 3 x 4 ## date n weekday weekend ## &lt;date&gt; &lt;dbl&gt; &lt;ord&gt; &lt;lgl&gt; ## 1 2016-01-01 0. Fri TRUE ## 2 2016-01-02 0. Sat TRUE ## 3 2016-01-03 1. Sun FALSE Now let’s calculate the probability that a day has at least one fatal crash, separately for weekends and weekdays: clark_co_accidents &lt;- clark_co_accidents %&gt;% mutate(any_crash = n &gt; 0) crash_prob &lt;- clark_co_accidents %&gt;% group_by(weekend) %&gt;% summarize(n_days = n(), crash_days = sum(any_crash)) %&gt;% mutate(prob_crash_day = crash_days / n_days) crash_prob ## # A tibble: 2 x 4 ## weekend n_days crash_days prob_crash_day ## &lt;lgl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 260 107 0.412 ## 2 TRUE 106 43 0.406 In R, you can use prop.test to test if two proportions are equal. Inputs include the total number of trials in each group (n =) and the number of “successes”&quot; (x =): prop.test(x = crash_prob$crash_days, n = crash_prob$n_days) ## ## 2-sample test for equality of proportions with continuity ## correction ## ## data: crash_prob$crash_days out of crash_prob$n_days ## X-squared = 1.5978e-30, df = 1, p-value = 1 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.1109757 0.1227318 ## sample estimates: ## prop 1 prop 2 ## 0.4115385 0.4056604 I won’t be teaching in this course how to find the correct statistical test. That’s something you’ll hopefully learn in a statistics course. There are also a variety of books that can help you with this, including some that you can access free online through CSU’s library. One servicable introduction is “Statistical Analysis with R for Dummies”. You can create an object from the output of any statistical test in R. Typically, this will be (at least at some level) in an object class called a “list”: vegas_test &lt;- prop.test(x = crash_prob$crash_days, n = crash_prob$n_days) is.list(vegas_test) ## [1] TRUE So far, we’ve mostly worked with two object types in R, dataframes and vectors. In the next subsection we’ll look more at two object classes we haven’t looked at much, matrices and lists. Both have important roles once you start applying more advanced methods to analyze your data. 7.2 Matrices A matrix is like a data frame, but all the values in all columns must be of the same class (e.g., numeric, character). R uses matrices a lot for its underlying math (e.g., for the linear algebra operations required for fitting regression models). R can do matrix operations quite quickly. You can create a matrix with the matrix function. Input a vector with the values to fill the matrix and ncol to set the number of columns: foo &lt;- matrix(1:10, ncol = 5) foo ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10 By default, the matrix will fill up by column. You can fill it by row with the byrow function: foo &lt;- matrix(1:10, ncol = 5, byrow = TRUE) foo ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 6 7 8 9 10 In certain situations, you might want to work with a matrix instead of a data frame (for example, in cases where you were concerned about speed – a matrix is more memory efficient than the corresponding data frame). If you want to convert a data frame to a matrix, you can use the as.matrix function: foo &lt;- data.frame(col_1 = 1:2, col_2 = 3:4, col_3 = 5:6, col_4 = 7:8, col_5 = 9:10) (foo &lt;- as.matrix(foo)) ## col_1 col_2 col_3 col_4 col_5 ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10 You can index matrices with square brackets, just like data frames: foo[1, 1:2] ## col_1 col_2 ## 1 3 You cannot, however, use dplyr functions with matrices: foo %&gt;% filter(col_1 == 1) All elements in a matrix must have the same class. The matrix will default to make all values the most general class of any of the values, in any column. For example, if we replaced one numeric value with the character “a”, everything would turn into a character: foo[1, 1] &lt;- &quot;a&quot; foo ## col_1 col_2 col_3 col_4 col_5 ## [1,] &quot;a&quot; &quot;3&quot; &quot;5&quot; &quot;7&quot; &quot;9&quot; ## [2,] &quot;2&quot; &quot;4&quot; &quot;6&quot; &quot;8&quot; &quot;10&quot; 7.3 Lists A list has different elements, just like a data frame has different columns. However, the different elements of a list can have different lengths (unlike the columns of a data frame). The different elements can also have different classes. bar &lt;- list(some_letters = letters[1:3], some_numbers = 1:5, some_logical_values = c(TRUE, FALSE)) bar ## $some_letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; ## ## $some_numbers ## [1] 1 2 3 4 5 ## ## $some_logical_values ## [1] TRUE FALSE To index an element from a list, use double square brackets. You can use bracket indexing either with numbers (which element in the list?) or with names. You can also index lists with the $ operator. bar[[1]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; bar[[&quot;some_numbers&quot;]] ## [1] 1 2 3 4 5 bar$some_logical_values ## [1] TRUE FALSE To access a specific value within a list element we can index the element e.g.: bar[[1]][[2]] ## [1] &quot;b&quot; Lists can be used to contain data with an unusual structure and / or lots of different components. For example, the information from fitting a regression is often stored as a list: my_mod &lt;- glm(rnorm(10) ~ c(1:10)) is.list(my_mod) ## [1] TRUE The names function returns the name of each element in the list: head(names(my_mod), 3) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;fitted.values&quot; my_mod[[&quot;coefficients&quot;]] ## (Intercept) c(1:10) ## -0.49878412 0.06078184 A list can even contain other lists. We can use the str function to see the structure of a list: a_list &lt;- list(list(&quot;a&quot;, &quot;b&quot;), list(1, 2)) str(a_list) ## List of 2 ## $ :List of 2 ## ..$ : chr &quot;a&quot; ## ..$ : chr &quot;b&quot; ## $ :List of 2 ## ..$ : num 1 ## ..$ : num 2 Sometimes you’ll see unnecessary lists-of-lists, perhaps when importing data into R created. Or a list with multiple elements that you would like to combine. You can remove a level of hierarchy from a list using the flatten function from the purrr package: library(purrr) a_list ## [[1]] ## [[1]][[1]] ## [1] &quot;a&quot; ## ## [[1]][[2]] ## [1] &quot;b&quot; ## ## ## [[2]] ## [[2]][[1]] ## [1] 1 ## ## [[2]][[2]] ## [1] 2 flatten(a_list) ## [[1]] ## [1] &quot;a&quot; ## ## [[2]] ## [1] &quot;b&quot; ## ## [[3]] ## [1] 1 ## ## [[4]] ## [1] 2 Let’s look at the list object from the statistical test we ran for Las Vegas: str(vegas_test) ## List of 9 ## $ statistic : Named num 1.6e-30 ## ..- attr(*, &quot;names&quot;)= chr &quot;X-squared&quot; ## $ parameter : Named num 1 ## ..- attr(*, &quot;names&quot;)= chr &quot;df&quot; ## $ p.value : num 1 ## $ estimate : Named num [1:2] 0.412 0.406 ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;prop 1&quot; &quot;prop 2&quot; ## $ null.value : NULL ## $ conf.int : atomic [1:2] -0.111 0.123 ## ..- attr(*, &quot;conf.level&quot;)= num 0.95 ## $ alternative: chr &quot;two.sided&quot; ## $ method : chr &quot;2-sample test for equality of proportions with continuity correction&quot; ## $ data.name : chr &quot;crash_prob$crash_days out of crash_prob$n_days&quot; ## - attr(*, &quot;class&quot;)= chr &quot;htest&quot; We can pull out an element using the $ notation: vegas_test$p.value ## [1] 1 Or using the [[ notation: vegas_test[[4]] ## prop 1 prop 2 ## 0.4115385 0.4056604 You may have noticed, though, that this output is not a tidy dataframe. Ack! That means we can’t use all the tidyverse tricks we’ve learned so far in the course! Fortunately, David Robinson noticed this problem and came up with a package called broom that can “tidy up” a lot of these kinds of objects. The broom package has three main functions: glance: Return a one-row, tidy dataframe from a model or other R object tidy: Return a tidy dataframe from a model or other R object augment: “Augment” the dataframe you input to the statistical function Here is the output for tidy for the vegas_test object (augment won’t work for this type of object, and glance gives the same thing as tidy): library(broom) tidy(vegas_test) ## estimate1 estimate2 statistic p.value parameter conf.low conf.high ## 1 0.4115385 0.4056604 1.597806e-30 1 1 -0.1109757 0.1227318 ## method ## 1 2-sample test for equality of proportions with continuity correction ## alternative ## 1 two.sided 7.4 Regression models 7.4.1 Formula structure Regression models can be used to estimate how the expected value of a dependent variable changes as independent variables change. In R, regression formulas take this structure: ## Generic code [response variable] ~ [indep. var. 1] + [indep. var. 2] + ... Notice that a tilde, ~, is used to separate the independent and dependent variables and that a plus sign, +, is used to join independent variables. This format mimics the statistical notation: \\[ Y_i \\sim X_1 + X_2 + X_3 \\] You will use this type of structure in R fo a lot of different function calls, including those for linear models (fit with the lm function) and generalized linear models (fit with the glm function). There are some conventions that can be used in R formulas. Common ones include: Convention Meaning I() evaluate the formula inside I() before fitting (e.g., I(x1 + x2)) : fit the interaction between x1 and x2 variables * fit the main effects and interaction for both variables (e.g., x1*x2 equals x1 + x2 + x1:x2) . include as independent variables all variables other than the response (e.g., y ~ .) 1 intercept (e.g., y ~ 1 for an intercept-only model) - do not include a variable in the data frame as an independent variables (e.g., y ~ . - x1); usually used in conjunction with . or 1 7.4.2 Linear models To fit a linear model, you can use the function lm(). This function is part of the stats package, which comes installed with base R. In this function, you can use the data option to specify the data frame from which to get the vectors. mod_a &lt;- lm(wt ~ ht, data = nepali) This previous call fits the model: \\[ Y_{i} = \\beta_{0} + \\beta_{1}X_{1,i} + \\epsilon_{i} \\] where: \\(Y_{i}\\) : weight of child \\(i\\) \\(X_{1,i}\\) : height of child \\(i\\) If you run the lm function without saving it as an object, R will fit the regression and print out the function call and the estimated model coefficients: lm(wt ~ ht, data = nepali) ## ## Call: ## lm(formula = wt ~ ht, data = nepali) ## ## Coefficients: ## (Intercept) ht ## -8.7581 0.2342 However, to be able to use the model later for things like predictions and model assessments, you should save the output of the function as an R object: mod_a &lt;- lm(wt ~ ht, data = nepali) This object has a special class, lm: class(mod_a) ## [1] &quot;lm&quot; This class is a special type of list object. If you use is.list to check, you can confirm that this object is a list: is.list(mod_a) ## [1] TRUE There are a number of functions that you can apply to an lm object. These include: Function Description summary Get a variety of information on the model, including coefficients and p-values for the coefficients coefficients Pull out just the coefficients for a model fitted Get the fitted values from the model (for the data used to fit the model) plot Create plots to help assess model assumptions residuals Get the model residuals For example, you can get the coefficients from the model by running: coefficients(mod_a) ## (Intercept) ht ## -8.7581466 0.2341969 The estimated coefficient for the intercept is always given under the name “(Intercept)”. Estimated coefficients for independent variables are given based on their column names in the original data (“ht” here, for \\(\\beta_1\\), or the estimated increase in expected weight for a one unit increase in height). You can use the output from a coefficients call to plot a regression line based on the model fit on top of points showing the original data (Figure 7.1). mod_coef &lt;- coefficients(mod_a) ggplot(nepali, aes(x = ht, y = wt)) + geom_point(size = 0.2) + xlab(&quot;Height (cm)&quot;) + ylab(&quot;Weight (kg)&quot;) + geom_abline(aes(intercept = mod_coef[1], slope = mod_coef[2]), col = &quot;blue&quot;) Figure 7.1: Example of using the output from a coefficients call to add a regression line to a scatterplot. You can also add a linear regression line to a scatterplot by adding the geom geom_smooth using the argument method = “lm”. You can use the function residuals on an lm object to pull out the residuals from the model fit: head(residuals(mod_a)) ## 1 2 3 4 6 7 ## 0.1993922 -0.4329393 -0.4373953 -0.1355300 -0.6749080 -1.0838199 The result of a residuals call is a vector with one element for each of the non-missing observations (rows) in the data frame you used to fit the model. Each value gives the different between the model fitted value and the observed value for each of these observations, in the same order the observations show up in the data frame. The residuals are in the same order as the observations in the original data frame. You can also use the shorter function coef as an alternative to coefficients and the shorter function resid as an alternative to residuals. As noted in the subsection on simple statistics functions, the summary function returns different output depending on the type of object that is input to the function. If you input a regression model object to summary, the function gives you a lot of information about the model. For example, here is the output returned by running summary for the linear regression model object we just created: summary(mod_a) ## ## Call: ## lm(formula = wt ~ ht, data = nepali) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.0077 -0.5479 -0.0293 0.4972 3.3514 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -8.758147 0.211529 -41.40 &lt;2e-16 *** ## ht 0.234197 0.002459 95.23 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8733 on 875 degrees of freedom ## (123 observations deleted due to missingness) ## Multiple R-squared: 0.912, Adjusted R-squared: 0.9119 ## F-statistic: 9068 on 1 and 875 DF, p-value: &lt; 2.2e-16 This output includes a lot of useful elements, including (1) basic summary statistics for the residuals (to meet model assumptions, the median should be around zero and the absolute values fairly similar for the first and third quantiles), (2) coefficient estimates, standard errors, and p-values, and (3) some model summary statistics, including residual standard error, degrees of freedom, number of missing observations, and F-statistic. The object returned by the summary() function when it is applied to an lm object is a list, which you can confirm using the is.list function: is.list(summary(mod_a)) ## [1] TRUE With any list, you can use the names function to get the names of all of the different elements of the object: names(summary(mod_a)) ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; &quot;na.action&quot; You can use the $ operator to pull out any element of the list. For example, to pull out the table with information on the estimated model coefficients, you can run: summary(mod_a)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -8.7581466 0.211529182 -41.40396 2.411051e-208 ## ht 0.2341969 0.002459347 95.22726 0.000000e+00 The plot function, like the summary function, will give different output depending on the class of the object that you input. For an lm object, you can use the plot function to get a number of useful diagnostic plots that will help you check regression assumptions (Figure 7.2): plot(mod_a) Figure 7.2: Example output from running the plot function with an lm object as the input. You can also use binary variables or factors as independent variables in regression models. For example, in the nepali dataset, sex is a factor variable with the levels “Male” and “Female”. You can fit a linear model of weight regressed on sex for this data with the call: mod_b &lt;- lm(wt ~ sex, data = nepali) This call fits the model: \\[ Y_{i} = \\beta_{0} + \\beta_{1}X_{1,i} + \\epsilon_{i} \\] where \\(X_{1,i}\\) : sex of child \\(i\\), where 0 = male and 1 = female. Here are the estimated coefficients from fitting this model: summary(mod_b)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.9092558 0.3099506 38.423076 5.130825e-190 ## sex -0.4866184 0.1982810 -2.454185 1.431429e-02 You’ll notice that, in addition to an estimated intercept ((Intercept)), the other estimated coefficient is sexFemale rather than just sex, although the column name in the data frame input to lm for this variable is sex. This is because, when a factor or binary variable is input as an independent variable in a linear regression model, R will fit an estimated coefficient for all levels of factors except the first factor level. By default, this first factor level is used as the baseline level, and so its estimated mean is given by the estimated intercept, while the other model coefficients give the estimated difference from this baseline. For example, the model fit above tells us that the estimated mean weight of males is 11.9, while the estimated mean weight of females is 11.9 + -0.5 = 11.4. 7.4.3 Generalized linear models (GLMs) You can fit a variety of models, including linear models, logistic models, and Poisson models, using generalized linear models (GLMs). For linear models, the only difference between lm and glm are the mechanics of how they estimate the model coefficients (lm uses least squares while glm uses maximum likelihood). You will (almost always) get exactly the same estimated coefficients regardless of whether you use glm or lm to fit a linear regression. For example, here is the code to fit a linear regression model for weight regressed on height from the nepali dataset: mod_c &lt;- glm(wt ~ ht, data = nepali) This call fits the same regression model I fit earlier with the lm function and saved as mod_a. You can see that the two methods give exactly the same coefficient estimates: coef(mod_c) ## (Intercept) ht ## -8.7581466 0.2341969 coef(mod_a) ## (Intercept) ht ## -8.7581466 0.2341969 Unlike the lm function, however, the glm function also allows you to fit other model types, including logistic and Poisson models. You can specify the model type using the family argument to the glm call: Model type family argument Linear family = gaussian(link = 'identity') Logistic family = binomial(link = 'logit') Poisson family = poisson(link = 'log') For example, say we wanted to fit a logistic regression for the nepali data of whether the probability of a child weighing more than 13 kg is associated with the child’s sex. First, create a binary variable in the nepali dataset, wt_over_13, that is TRUE if a child weighed more than 13 kilograms and FALSE otherwise. You can use the mutate function from dplyr to add this new column (which, as a note, is a logical vector): nepali &lt;- nepali %&gt;% mutate(wt_over_13 = wt &gt; 13) head(nepali) ## id sex wt ht mage lit died alive age wt_over_13 ## 1 120011 1 12.8 91.2 35 0 2 5 41 FALSE ## 2 120011 1 12.8 93.9 35 0 2 5 45 FALSE ## 3 120011 1 13.1 95.2 35 0 2 5 49 TRUE ## 4 120011 1 13.8 96.9 35 0 2 5 53 TRUE ## 5 120011 1 NA NA 35 0 2 5 57 NA ## 6 120012 2 14.9 103.9 35 0 2 5 57 TRUE Now you can fit a logistic regression of wt_over_13 regressed on sex, using a logistic model: mod_d &lt;- glm(wt_over_13 ~ sex, data = nepali, family = binomial(link = &quot;logit&quot;)) Elements of a GLM can be pulled out in the same way that we looked at elements from the linear model fit with lm. For example, to see a table of estimated model coefficients, you can run: summary(mod_d)$coef ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.6038545 0.2350333 -2.569229 0.01019250 ## sex -0.2697469 0.1533194 -1.759378 0.07851325 Because this model was a logistic model, fit with a log link, here the model coefficient estimate for sexFemale gives an estimate of the log odds of weight higher than 13 kg associated with females versus males. The p-value for this estimate (Pr(&gt;|z|) = 0.08) isn’t very small, suggesting that the difference between male and female children in the odds of weighing more than 13 kg is not statistically significant. 7.5 Handling model objects The broom package contains tools for converting statistical objects into nice tidy data frames. The tools in the broom package make it easy to process statistical results in R using the tools of the tidyverse. 7.5.1 broom::tidy The tidy() function returns a data frame with information on the fitted model terms. For example, when applied to one of our linear models we get: library(broom) kable(tidy(mod_a), digits = 3) term estimate std.error statistic p.value (Intercept) -8.758 0.212 -41.404 0 ht 0.234 0.002 95.227 0 class(tidy(mod_a)) ## [1] &quot;data.frame&quot; You can pass arguments to the tidy() function. For example, include confidence intervals: kable(tidy(mod_a, conf.int = TRUE), digits = 3) term estimate std.error statistic p.value conf.low conf.high (Intercept) -8.758 0.212 -41.404 0 -9.173 -8.343 ht 0.234 0.002 95.227 0 0.229 0.239 7.5.2 broom::augment The augment() function adds information about a fitted model to the dataset used to fit the model. For example, when applied to one of our linear models we get information on the fitted values and residuals included in the output: kable(head(broom::augment(mod_a), 3), digits = 3) .rownames wt ht .fitted .se.fit .resid .hat .sigma .cooksd .std.resid 1 12.8 91.2 12.601 0.033 0.199 0.001 0.874 0 0.228 2 12.8 93.9 13.233 0.036 -0.433 0.002 0.874 0 -0.496 3 13.1 95.2 13.537 0.038 -0.437 0.002 0.874 0 -0.501 7.5.3 broom::glance The glance() functions returns a one row summary of a fitted model object: For example: kable(glance(mod_a, conf.int = TRUE), digits = 3) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual 0.912 0.912 0.873 9068.23 0 2 -1124.615 2255.23 2269.56 667.351 875 7.5.4 References– statistics in R One great (and free online for CSU students through our library) book to find out more about using R for basic statistics is: Introductory Statistics with R If you want all the details about fitting linear models and GLMs in R, Julian Faraway’s books are fantastic. He has one on linear models and one on extensions including logistic and Poisson models: Linear Models with R (also free online through the CSU library) Extending the Linear Model with R 7.6 In-course exercise 7.6.1 Running a simple statistical test In last week’s in-course exercise, we found out that about 17% of babies born in the United States between 1980 and 1995 had names that started with an “A” or “K” (60,893 babies out of 359,011). What is the proportion of people with names that start with an “A” or “K” in our class? Use a simple statistical test to test the hypothesis that the class comes from a binomial distribution with the same distribution as babies born in the US between 1980 and 1995, in terms of chance of having a name that starts with “A” or “K”. You may get the warning “Chi-squared approximation may be incorrect”. See if you can figure out this warning. If you are able to figure out and run a statistical test, go back to your notes from last week and write a full script, starting from loading the babynames data and creating your own vector of the names in this class, all the way through to running the statistical test you picked. 7.6.1.1 Example R code Here is a vector with names in our class: library(stringr) student_list &lt;- data_frame(name = c(&quot;Aeriel&quot;, &quot;Rebecca&quot;, &quot;Grant&quot;, &quot;Amy&quot;, &quot;Jessy&quot;, &quot;Alyssa&quot;, &quot;Camron&quot;, &quot;Anastasia&quot;, &quot;Kyle&quot;, &quot;Ana&quot;, &quot;Amanda&quot;, &quot;Kathleen&quot;, &quot;Kyle&quot;, &quot;Ana&quot;, &quot;Amanda&quot;, &quot;Kathleen&quot;, &quot;Kayla&quot;, &quot;Nichole&quot;, &quot;Randy&quot;, &quot;Katy&quot;, &quot;Devin&quot;)) student_list &lt;- student_list %&gt;% mutate(first_letter = str_sub(name, 1, 1)) student_list %&gt;% slice(1:3) ## # A tibble: 3 x 2 ## name first_letter ## &lt;chr&gt; &lt;chr&gt; ## 1 Aeriel A ## 2 Rebecca R ## 3 Grant G Let’s get the total number of students, and then the total number with a name that starts with “A” or “K”: tot_students &lt;- student_list %&gt;% count() tot_students ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 21 a_or_k_students &lt;- student_list %&gt;% mutate(a_or_k = first_letter %in% c(&quot;A&quot;, &quot;K&quot;)) %&gt;% group_by(a_or_k) %&gt;% count() a_or_k_students ## # A tibble: 2 x 2 ## # Groups: a_or_k [2] ## a_or_k n ## &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 7 ## 2 TRUE 14 The proportion of students with names starting with “A” or “K” are 14 / 21 = 0.67. You could run a statistical test comparing these two proportions: prop.test(x = c(60893, 14), n = c(359011, 21)) ## Warning in prop.test(x = c(60893, 14), n = c(359011, 21)): Chi-squared ## approximation may be incorrect ## ## 2-sample test for equality of proportions with continuity ## correction ## ## data: c(60893, 14) out of c(359011, 21) ## X-squared = 33.386, df = 1, p-value = 7.557e-09 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.7224875 -0.2716195 ## sample estimates: ## prop 1 prop 2 ## 0.1696132 0.6666667 You could also test whether the proportion in our class is consistent with the null hypothesis that you were drawn from a binomial distribution with a proportion of 0.17 (in-line with the national values): prop.test(x = 14, n = 21, p = 0.17) ## Warning in prop.test(x = 14, n = 21, p = 0.17): Chi-squared approximation ## may be incorrect ## ## 1-sample proportions test with continuity correction ## ## data: 14 out of 21, null probability 0.17 ## X-squared = 33.278, df = 1, p-value = 7.99e-09 ## alternative hypothesis: true p is not equal to 0.17 ## 95 percent confidence interval: ## 0.4310506 0.8451865 ## sample estimates: ## p ## 0.6666667 Finally, when we run this test, we get the warning that “Chi-squared approximation may be incorrect”. Based on googling ‘r prop.test “Chi-squared approximation may be incorrect”’, it sounds like we might be getting this error because we have a pretty low number of people in the class. One recommendation is to instead run a Chi-square contingency table test, which you can do with chisq.test, while setting simulate.p.values to TRUE. This requires a slightly different set-up for the data (which you can figure out by trying out the examples in the helpfile for chisq.test): a_or_k_names &lt;- matrix(c(14, 60893, 7, 298118), nrow = 2) a_or_k_names ## [,1] [,2] ## [1,] 14 7 ## [2,] 60893 298118 chisq.test(a_or_k_names, simulate.p.value = TRUE) ## ## Pearson&#39;s Chi-squared test with simulated p-value (based on 2000 ## replicates) ## ## data: a_or_k_names ## X-squared = 36.83, df = NA, p-value = 0.0004998 You could also use binom.test, which will run as an exact binomial test: binom.test(x = 14, n = 21, p = 0.17) ## ## Exact binomial test ## ## data: 14 and 21 ## number of successes = 14, number of trials = 21, p-value = ## 5.862e-07 ## alternative hypothesis: true probability of success is not equal to 0.17 ## 95 percent confidence interval: ## 0.4303245 0.8541231 ## sample estimates: ## probability of success ## 0.6666667 7.6.2 Using regression models to explore data #1 For this exercise, you will need the following packages. If do not have them already, you will need to install them. library(ggplot2) library(broom) library(ggfortify) For this part of the exercise, you’ll use a dataset on weather, air pollution, and mortality counts in Chicago, IL. This dataset is called chicagoNMMAPS and is part of the dlnm package. Change the name of the data frame to chic (this object name is shorter and will be easier to work with). Check out the data a bit to see what variables you have, and then perform the following tasks: Write out (on paper, not in R) the regression equation for regressing dewpoint temperature on temperature. Try fitting a linear regression of dew point temperature (dptp) regressed on temperature (temp). Save this model as the object mod_1 (i.e., is the dependent variable of dewpoint temperature linearly associated with the independent variable of temperature). Based on this regression, does there seem to be a relationship between temperature and dewpoint temperature in Chicago? (Hint: Try using glance and tidy from the broom package on the model object to get more information about the model you fit.) What is the coefficient for temperature (in other words, for every 1 degree increase in temperature, how much do we expect the dewpoint temperature to change?)? What is the p-value for the coefficient for temperature? Plot temperature (x-axis) versus dewpoint temperature (y-axis) for Chicago. Add in the regression line from the model you fit by using the results from augment. Use autoplot on the model object to generate some model diagnostic plots (make sure you have the ggfortify package loaded and installed). Try fitting the regression as a GLM, using glm() (but still assuming the outcome variable is normally distributed). Are your coefficients different? 7.6.2.1 Example R code: The regression equation for the model you want to fit, regressing dewpoint temperature on temperature, is: \\[ Y_t \\sim \\beta_0 + \\beta_1 X_t \\] where \\(Y_t\\) is the dewpoint temperature on day \\(t\\), \\(X_t\\) is the temperature on day \\(t\\), and \\(\\beta_0\\) and \\(\\beta_1\\) are model coefficients. Install and load the dlnm package and then load the chicagoNMMAPS data. Change the name of the data frame to chic, so it will be shorter to call for the rest of your work. # install.packages(&quot;dlnm&quot;) library(dlnm) data(&quot;chicagoNMMAPS&quot;) chic &lt;- chicagoNMMAPS Fit a linear regression of dptp on temp and save as the object mod_1: mod_1 &lt;- lm(dptp ~ temp, data = chic) mod_1 ## ## Call: ## lm(formula = dptp ~ temp, data = chic) ## ## Coefficients: ## (Intercept) temp ## 24.025 1.621 Use functions from the broom package to pull the same information about the model in a “tidy” format. To find out if the evidence for a linear association between temperature and dewpoint temperature, use the tidy function to get model coefficients in a tidy format: tidy(mod_1) ## term estimate std.error statistic p.value ## 1 (Intercept) 24.02487 0.112933468 212.7347 0 ## 2 temp 1.62065 0.007630629 212.3875 0 There does seem to be an association between temperature and dewpoint temperature: a unit increase in temperature is associated with a 1.6 unit increase in dewpoint temperature. The p-value for the temperature coefficient is &lt;2e-16. This is far below 0.05, which suggests we would be very unlikely to see such a strong association by chance if the null hypothesis, that the two variables are not associated, were true. You can also check overall model summaries using the glance function: glance(mod_1) ## r.squared adj.r.squared sigma statistic p.value df logLik AIC ## 1 0.8982088 0.8981888 5.899475 45108.43 0 2 -16332.1 32670.21 ## BIC deviance df.residual ## 1 32689.82 177917.1 5112 To create plots of the observations and the fit model, use the augment function to add model output (e.g., predictions, residuals) to the original data frame of observed temperatures and dew point temperatures: augment(mod_1) %&gt;% slice(1:3) ## # A tibble: 3 x 9 ## dptp temp .fitted .se.fit .resid .hat .sigma .cooksd .std.resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 31.5 -0.278 23.6 0.114 7.93 0.000376 5.90 0.000340 1.34 ## 2 29.9 0.556 24.9 0.110 4.95 0.000348 5.90 0.000123 0.839 ## 3 27.4 0.556 24.9 0.110 2.45 0.000348 5.90 0.0000300 0.415 Plot these two variables and add in the fitted line from the model (note: I’ve used the color option to make the color of the points gray). Use the output from augment to create a plot of the original data, with the predicted values used to plot a fitted line. augment(mod_1) %&gt;% ggplot(aes(x = temp, y = dptp)) + geom_point(size = 0.8, alpha = 0.5, col = &quot;gray&quot;) + geom_line(aes(x = temp, y = .fitted), color = &quot;red&quot;, size = 2) + theme_classic() Plot some plots to check model assumptions for the model you fit using the autoplot function on your model object: autoplot(mod_1) Try fitting the model using glm(). Call it mod_1a. Compare the coefficients for the two models. You can use the tidy function on an lm or glm object to pull out just the model coefficients and associated model results. Here, I’ve used a pipeline of code to create a tidy data frame that merges these “tidy” coefficient outputs (from the two models) into a single data frame): mod_1a &lt;- glm(dptp ~ temp, data = chic) tidy(mod_1) %&gt;% select(term, estimate) %&gt;% inner_join(mod_1a %&gt;% tidy() %&gt;% select(term, estimate), by = &quot;term&quot;) %&gt;% rename(estimate_lm_mod = estimate.x, estimate_glm_mod = estimate.y) ## term estimate_lm_mod estimate_glm_mod ## 1 (Intercept) 24.02487 24.02487 ## 2 temp 1.62065 1.62065 The results from the two models are identical. As a note, you could have also just run tidy on each model object, without merging them together into a single data frame: tidy(mod_1) ## term estimate std.error statistic p.value ## 1 (Intercept) 24.02487 0.112933468 212.7347 0 ## 2 temp 1.62065 0.007630629 212.3875 0 tidy(mod_1a) ## term estimate std.error statistic p.value ## 1 (Intercept) 24.02487 0.112933468 212.7347 0 ## 2 temp 1.62065 0.007630629 212.3875 0 7.6.3 Using regression models to explore data #2 Does \\(PM_{10}\\) vary by day of the week? (Hint: The dow variable is a factor that gives day of the week. You can do an ANOVA analysis by fitting a linear model using this variable as the independent variable. Some of the overall model summaries will compare this model to an intercept-only model.) What day of the week is PM10 generally highest? (Check the model coefficients to figure this out.) Try to write out (on paper) the regression equation for the model you’re fitting. Try using glm() to run a Poisson regression of respiratory deaths (resp) on temperature during summer days. Start by creating a subset with just summer days called summer. (Hint: Use the month function with the argument label = TRUE from lubridate to do this– just pull out the subset where the month is 6, 7, or 8, for “Jun”, “Jul”, and “Aug”.) Try to write out the regression equation for the model you’re fitting. The coefficient for the temperature variable in this model is our best estimate (based on this model) of the log relative risk for a one degree Celcius increase in temperature. What is the relative risk associated with a one degree Celsius increase? 7.6.3.1 Example R code: Fit a model of \\(PM_{10}\\) regressed on day of week, where day of week is a factor. mod_2 &lt;- lm(pm10 ~ dow, data = chic) tidy(mod_2) ## term estimate std.error statistic p.value ## 1 (Intercept) 27.521671 0.7303211 37.684344 7.467602e-273 ## 2 dowMonday 6.132236 1.0339702 5.930767 3.224025e-09 ## 3 dowTuesday 6.795433 1.0268941 6.617462 4.048930e-11 ## 4 dowWednesday 8.476816 1.0261689 8.260644 1.850086e-16 ## 5 dowThursday 8.804654 1.0240148 8.598171 1.078208e-17 ## 6 dowFriday 9.481589 1.0261689 9.239794 3.609870e-20 ## 7 dowSaturday 3.660201 1.0268941 3.564342 3.682785e-04 Use glance to check some of the overall summaries of this model. The statistic column here is the F statistic from test comparing this model to an intercept-only model. glance(mod_2) ## r.squared adj.r.squared sigma statistic p.value df logLik ## 1 0.02587711 0.0246735 19.07243 21.49955 4.607646e-25 7 -21234.11 ## AIC BIC deviance df.residual ## 1 42484.21 42536.13 1766407 4856 As a note, you may have heard in previous statistics classes that you can use the anova() command to compare this model to a model with only an intercept (i.e., one that only fits a global mean and uses that as the expected value for all of the observations). Note that, in this case, the F value from anova for this model comparison is the same as the statistic you got in the overall summary statistics you get with glance in the previous code. anova(mod_2) ## Analysis of Variance Table ## ## Response: pm10 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## dow 6 46924 7820.6 21.5 &lt; 2.2e-16 *** ## Residuals 4856 1766407 363.8 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The overall p-value from anova for with day-of-week coefficients versus the model that just has an intercept is &lt; 2.2e-16. This is well below 0.05, which suggests that day-of-week is associated with PM10 concentration, as a model that includes day-of-week does a much better job of explaining variation in PM10 than a model without it does. Use a boxplot to visually compare PM10 by day of week. ggplot(chic, aes(x = dow, y = pm10)) + geom_boxplot() Now try the same plot, but try using the ylim = option to change the limits on the y-axis for the graph, so you can get a better idea of the pattern by day of week (some of the extreme values are very high, which makes it hard to compare by eye when the y-axis extends to include them all). ggplot(chic, aes(x = dow, y = pm10)) + geom_boxplot() + ylim(c(0, 100)) ## Warning: Removed 292 rows containing non-finite values (stat_boxplot). Create a subset called summer with just the summer days: library(lubridate) summer &lt;- chic %&gt;% mutate(month = month(date, label = TRUE)) %&gt;% filter(month %in% c(&quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;)) summer %&gt;% slice(1:3) ## # A tibble: 3 x 14 ## date time year month doy dow death cvd resp temp dptp ## &lt;date&gt; &lt;int&gt; &lt;dbl&gt; &lt;ord&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1987-06-01 152 1987. Jun 152 Monday 112 60 5 23.6 68.5 ## 2 1987-06-02 153 1987. Jun 153 Tuesday 111 57 7 22.2 64.8 ## 3 1987-06-03 154 1987. Jun 154 Wednes… 120 59 9 20.6 47.2 ## # ... with 3 more variables: rhum &lt;dbl&gt;, pm10 &lt;dbl&gt;, o3 &lt;dbl&gt; Use glm() to fit a Poisson model of respiratory deaths regressed on temperature. Since you want to fit a Poisson model, use the option family = poisson(link = &quot;log&quot;). mod_3 &lt;- glm(resp ~ temp, data = summer, family = poisson(link = &quot;log&quot;)) glance(mod_3) ## null.deviance df.null logLik AIC BIC deviance df.residual ## 1 1499.417 1287 -3210.68 6425.36 6435.682 1493.753 1286 tidy(mod_3) ## term estimate std.error statistic p.value ## 1 (Intercept) 1.910316958 0.058372529 32.726301 6.600945e-235 ## 2 temp 0.006136743 0.002580526 2.378098 1.740221e-02 Use the fitted model coefficient to determine the relative risk for a one degree Celsius increase in temperature. First, remember that you can use the tidy() function to read out the model coefficients. The second of these is the value for the temperature coefficient. That means that you can use indexing ([2]) to get just that value. That’s the log relative risk; take the exponent to get the relative risk. tidy(mod_3) %&gt;% filter(term == &quot;temp&quot;) %&gt;% mutate(log_rr = exp(estimate)) ## term estimate std.error statistic p.value log_rr ## 1 temp 0.006136743 0.002580526 2.378098 0.01740221 1.006156 As a note, you can use the conf.int parameter in tidy to also pull confidence intervals: tidy(mod_3, conf.int = TRUE) ## term estimate std.error statistic p.value conf.low ## 1 (Intercept) 1.910316958 0.058372529 32.726301 6.600945e-235 1.795647330 ## 2 temp 0.006136743 0.002580526 2.378098 1.740221e-02 0.001082325 ## conf.high ## 1 2.02446414 ## 2 0.01119783 You could use this to get the confidence interval for relative risk (check out the mutate_at function if you haven’t seen it before): tidy(mod_3, conf.int = TRUE) %&gt;% select(term, estimate, conf.low, conf.high) %&gt;% filter(term == &quot;temp&quot;) %&gt;% mutate_at(vars(estimate:conf.high), funs(exp(.))) ## term estimate conf.low conf.high ## 1 temp 1.006156 1.001083 1.011261 7.6.4 Exploring Fatality Analysis Reporting System (FARS) data Visit http://metrocosm.com/10-years-of-traffic-accidents-mapped.html and explore the interactive visualization created by Max Galka using this public dataset on US fatal motor vehicle accidents. Go to FARS web page. We want to get the raw data on fatal accidents. Navigate this page to figure out how you can get this raw data for the whole county for 2015 (hint: you’ll need to access the raw data using FTP, and you may have more success with some web browsers that others). Save 2015 “National” data (csv format) to your computer. What is the structure of how this data is saved (e.g., directory structure, file structure)? On the FARS web page, find the documentation describing this raw data. (The relevant documentation file is called Fatality Analysis Reporting System (FARS) Analytical User’s Manual 1975-2015) Look through both this documentation and the raw files you downloaded to figure out what information is included in the data. Read the accident.csv file for 2015 into R (this is one of the files you’ll get if you download the raw data for 2015). Use the documentation to figure out what each column represents. Discuss what steps you would need to take to create the following plot. To start, don’t write any code, just develop a plan. Talk about what the dataset should look like right before you create the plot and what functions you could use to get the data from its current format to that format. (Hint: Functions from the lubridate package will be very helpful, including yday and wday). Discuss which of the variables in this dataset could be used to merge the dataset with other appropriate data, either other datasets in the FARS raw data, or outside datasets. Try to write the code to create the plot below. This will include some code for cleaning the data and some code for plotting. There is an example answer below, but I’d like you to try to figure it out yourselves first. 7.6.4.1 Example R code Here is example code for the section above: library(tidyverse) library(lubridate) library(ggthemes) accident &lt;- read_csv(&quot;data/accident.csv&quot;) %&gt;% select(DAY:MINUTE) %&gt;% select(-DAY_WEEK) %&gt;% unite(date, DAY:MINUTE, sep = &quot;-&quot;, remove = FALSE) %&gt;% mutate(date = dmy_hm(date), yday = yday(date), weekday = wday(date, label = TRUE, abbr = FALSE), weekend = weekday %in% c(&quot;Saturday&quot;, &quot;Sunday&quot;)) accident %&gt;% filter(!is.na(yday)) %&gt;% group_by(yday) %&gt;% summarize(accidents = n(), weekend = first(weekend)) %&gt;% ggplot(aes(x = yday, y = accidents, color = weekend)) + geom_point(alpha = 0.5) + xlab(&quot;Day of the year in 2015&quot;) + ylab(&quot;# of fatal accidents&quot;) + theme_few() + geom_smooth(se = FALSE) "]
]
